#  Shipping a Data Product: From Raw Telegram Data to Analytical API & Dashboard

A comprehensive data engineering project demonstrating the full lifecycle of building a production-ready data product â€” from raw unstructured data collection to a dimensional warehouse, enrichment with AI, an analytical API, and transparent explainability features.

---

## ðŸ“‹ Project Overview

This project ingests raw messages from Ethiopian medical business Telegram channels, enriches them with **YOLOv8 object detection**, structures them into a **dbt-powered dimensional warehouse**, and exposes insights through both:
- a **FastAPI analytical API** (Week 7)
- a **Streamlit dashboard for transparency and explainability** (Week 12)

The pipeline is orchestrated with **Dagster** and tested continuously with **CI/CD (GitHub Actions + pytest)**.

---

##  Repository Structure

```bash
Shipping-a-Data-Product/
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for prototyping & EDA
â”‚   â”œâ”€â”€ 01_bootstrap_session.ipynb
â”‚   â”œâ”€â”€ 02_scrape_channels.ipynb
â”‚   â”œâ”€â”€ 03_load_to_postgres.ipynb
â”‚   â””â”€â”€ 04_yolo_enrichment.ipynb
â”‚
â”œâ”€â”€ scripts/                     # ETL scripts (production-ready)
â”‚   â”œâ”€â”€ scrape.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ enrich.py
â”‚
â”œâ”€â”€ src/                         # Core Python package
â”‚   â”œâ”€â”€ utils/                   # Configs & helpers
â”‚   â”œâ”€â”€ ingestion/               # Data ingestion logic
â”‚   â”œâ”€â”€ enrichment/              # YOLO object detection
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ dbt/                         # Data transformation layer
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ api/                         # FastAPI app
â”‚   â”œâ”€â”€ main.py                  # API entry point
â”‚   â”œâ”€â”€ crud.py                  # Database queries
â”‚   â”œâ”€â”€ schemas.py               # Pydantic models
â”‚   â”œâ”€â”€ database.py              # DB session config
â”‚   â””â”€â”€ routers/                 # Modular endpoints
â”‚
â”œâ”€â”€ streamlit_app.py             # Streamlit dashboard (Week 12)
â”‚
â”œâ”€â”€ dagster_repo/                # Orchestration layer
â”‚   â”œâ”€â”€ repository.py
â”‚   â””â”€â”€ jobs.py
â”‚
â”œâ”€â”€ tests/                       # Unit & integration tests
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_enrichment.py
â”‚   â””â”€â”€ test_environment.py
â”‚
â”œâ”€â”€ .github/workflows/ci.yml     # CI/CD pipeline
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                 # Environment variables template
â””â”€â”€ README.md                    # Project documentation
````

---

## Pipeline Workflow

### **1. Data Ingestion**

* Scrape Telegram messages and images (`scripts/scrape.py`).
* Load into PostgreSQL (`raw_telegram_messages`).

### **2. Data Transformation (dbt)**

* Staging models for cleaning/normalization.
* Mart models for analytics queries.
* Run with:

  ```bash
  cd dbt
  dbt run --profiles-dir .
  dbt test --profiles-dir .
  ```

### **3. Image Enrichment**

* YOLOv8 detection on scraped images.
* Store detections in `image_detections` table.

### **4. API Service (Week 7)**

* Exposes insights via FastAPI.
* Docs: `http://127.0.0.1:8000/docs`
* Run with:

  ```bash
  uvicorn api.main:app --reload
  ```

### **5. Orchestration**

* Pipelines scheduled and monitored via Dagster.
* Start dashboard:

  ```bash
  dagit -w workspace.yaml
  ```

### **6. Transparency & Dashboard (Week 12)**

* Streamlit dashboard to visualize ingestion trends and detections.
* Run with:

  ```bash
  streamlit run streamlit_app.py
  ```

---

## API Endpoints

| Endpoint                           | Method | Description                            |
| ---------------------------------- | ------ | -------------------------------------- |
| `/api/health`                      | GET    | Service health check                   |
| `/api/reports/top-products`        | GET    | Top mentioned products                 |
| `/api/channels/{channel}/activity` | GET    | Channel activity over time             |
| `/api/search/messages`             | GET    | Full-text search                       |
| `/api/metrics/ingestion`           | GET    | Messages ingested/day (last 14 days)   |
| `/api/metrics/detections`          | GET    | Object detection counts (last 14 days) |

---

##  Tech Stack

* **ETL & Processing**: Python, Pandas, SQLAlchemy
* **Database**: PostgreSQL
* **Data Modeling**: dbt
* **API**: FastAPI
* **Visualization**: Streamlit
* **Orchestration**: Dagster
* **Computer Vision**: YOLOv8 (Ultralytics)
* **CI/CD**: GitHub Actions + pytest

---

##  Week-by-Week

### **Week 7 â€“ Foundations**

* Telegram scraping + ingestion
* dbt warehouse modeling
* YOLOv8 enrichment
* FastAPI analytical endpoints

### **Week 12 â€“ Transparency**

* New explainability endpoints
* Streamlit dashboard
* CI/CD with GitHub Actions

---

## Quick Start

1. **Clone repo**

   ```bash
   git clone https://github.com/<your-username>/Shipping-a-Data-Product.git
   cd Shipping-a-Data-Product
   ```
2. **Setup environment**

   ```bash
   python -m venv .venv
   .venv\Scripts\activate   # Windows
   source .venv/bin/activate # Linux/Mac
   pip install -r requirements.txt
   ```
3. **Configure `.env`**
   Copy `.env.example` â†’ `.env` and fill in credentials.
4. **Run API**

   ```bash
   uvicorn api.main:app --reload
   ```
5. **Run Dashboard**

   ```bash
   streamlit run streamlit_app.py
   ```


